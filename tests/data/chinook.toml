machine = "chinook"

[MPI]

mpido = "mpirun -np {cores} -machinefile ./nodes_$SLURM_JOBID"

[scheduler]

name = "SLRUM"
submit = "sbatch"
job_id =  "SLURM_JOBID"

[filesystem]

work_dir = "SLURM_SUBMIT_DIR"

[queues]

t1standard = 24
t1small = 24
t2standard =  24
t2small = 24
debug = 24
analysis = 24

[job]

header =  """#!/bin/sh
#SBATCH --partition={queue}
#SBATCH --ntasks={cores}
#SBATCH --tasks-per-node={ppn}
#SBATCH --time={walltime}
#SBATCH --mail-type=BEGIN
#SBATCH --mail-type=END
#SBATCH --mail-type=FAIL
#SBATCH --output=pism.%j

module list

umask 007

cd $SLURM_SUBMIT_DIR

# Generate a list of compute node hostnames reserved for this job,
# this ./nodes file is necessary for slurm to spawn mpi processes
# across multiple compute nodes
srun -l /bin/hostname | sort -n | awk '{{print $2}}' > ./nodes_$SLURM_JOBID

ulimit -l unlimited
ulimit -s unlimited
ulimit

"""
