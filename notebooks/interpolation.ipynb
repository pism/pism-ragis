{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a86d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import xarray as xr\n",
    "from pismragis.trajectories import compute_trajectory, compute_perturbation\n",
    "from joblib import Parallel, delayed\n",
    "import geopandas as gp\n",
    "from pathlib import Path\n",
    "from pismragis.processing import tqdm_joblib\n",
    "import pandas as pd\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import uniform\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb74282",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(\"~/Google Drive/My Drive/data/ITS_LIVE/GRE_G0240_0000.nc\")\n",
    "data_url = Path(\"~/Google Drive/My Drive/data/ITS_LIVE/GRE_G0240_0000.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c055319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ogr_url = Path(\"/Users/andy/Google Drive/My Drive/data/GreenlandFluxGatesAschwanden/greenland-flux-gates-jibneighbors.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf859ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "VX = np.squeeze(ds[\"vx\"].to_numpy())\n",
    "VY = np.squeeze(ds[\"vy\"].to_numpy())\n",
    "VX_e = np.squeeze(ds[\"vx_err\"].to_numpy())\n",
    "VY_e = np.squeeze(ds[\"vy_err\"].to_numpy())\n",
    "\n",
    "x = ds[\"x\"].to_numpy()\n",
    "y = ds[\"y\"].to_numpy()\n",
    "nx = len(x)\n",
    "ny = len(y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90d538",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draw_samples = 20\n",
    "unif_sample = lhs(2, n_draw_samples)\n",
    "\n",
    "with tqdm_joblib(tqdm(desc=\"Processing perturbations\", total=n_draw_samples, position=0)) as progress_bar:\n",
    "    all_perturb_glaciers = Parallel(n_jobs=10)(\n",
    "        delayed(compute_perturbation)(data_url, ogr_url,\n",
    "                                       perturbation=perturb, sample=unif_sample[perturb, :], \n",
    "                                       total_time=1_000, dt=1, reverse=True)\n",
    "                for perturb in range(n_draw_samples))\n",
    "    del progress_bar\n",
    "\n",
    "all_perturb_glaciers = pd.concat(all_perturb_glaciers).reset_index(drop=True)\n",
    "# all_perturb_glaciers.to_file(\"all_perturb_traj.gpkg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.argmax?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043bafec",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_perturbations = 10\n",
    "unif_sample = lhs(2, n_perturbations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8fc27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import geopandas as gp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame\n",
    "from numpy import ndarray\n",
    "from osgeo import ogr, osr\n",
    "from shapely import Point\n",
    "from tqdm.auto import tqdm\n",
    "from xarray import DataArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf37944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perturbation(\n",
    "    data_url: Union[str, Path],\n",
    "    ogr_url: Union[str, Path],\n",
    "    perturbation: int = 0,\n",
    "    sample: Union[list, ndarray] = [0.5, 0.5],\n",
    "    sigma: float = 1,\n",
    "    total_time: float = 10_000,\n",
    "    dt: float = 1,\n",
    "    reverse: bool = False,\n",
    ") -> GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Compute a perturbed trajectory.\n",
    "\n",
    "    It appears OGR objects cannot be pickled by joblib hence we load it here.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string or pathlib.Path\n",
    "        Path to an ogr data set\n",
    "    VX_min : numpy.ndarray or xarray.DataArray\n",
    "        Minimum\n",
    "    VX_min : dict-like, optional\n",
    "        Another mapping in similar form as the `data_vars` argument,\n",
    "        except the each item is saved on the dataset as a \"coordinate\".\n",
    "        These variables have an associated meaning: they describe\n",
    "        constant/fixed/independent quantities, unlike the\n",
    "        varying/measured/dependent quantities that belong in\n",
    "        `variables`. Coordinates values may be given by 1-dimensional\n",
    "        arrays or scalars, in which case `dims` do not need to be\n",
    "        supplied: 1D arrays will be assumed to give index values along\n",
    "        the dimension with the same name.\n",
    "\n",
    "        The following notations are accepted:\n",
    "\n",
    "        - mapping {coord name: DataArray}\n",
    "        - mapping {coord name: Variable}\n",
    "        - mapping {coord name: (dimension name, array-like)}\n",
    "        - mapping {coord name: (tuple of dimension names, array-like)}\n",
    "        - mapping {dimension name: array-like}\n",
    "          (the dimension name is implicitly set to be the same as the\n",
    "          coord name)\n",
    "\n",
    "        The last notation implies that the coord name is the same as\n",
    "        the dimension name.\n",
    "\n",
    "    attrs : dict-like, optional\n",
    "        Global attributes to save on this dataset.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Create data:\n",
    "\n",
    "    >>> np.random.seed(0)\n",
    "    >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n",
    "    >>> precipitation = 10 * np.random.rand(2, 2, 3)\n",
    "    >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n",
    "    >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n",
    "    >>> time = pd.date_range(\"2014-09-06\", periods=3)\n",
    "    >>> reference_time = pd.Timestamp(\"2014-09-05\")\n",
    "\n",
    "    Initialize a dataset with multiple dimensions:\n",
    "\n",
    "    >>> ds = xr.Dataset(\n",
    "    ...     data_vars=dict(\n",
    "    ...         temperature=([\"x\", \"y\", \"time\"], temperature),\n",
    "    ...         precipitation=([\"x\", \"y\", \"time\"], precipitation),\n",
    "    ...     ),\n",
    "    ...     coords=dict(\n",
    "    ...         lon=([\"x\", \"y\"], lon),\n",
    "    ...         lat=([\"x\", \"y\"], lat),\n",
    "    ...         time=time,\n",
    "    ...         reference_time=reference_time,\n",
    "    ...     ),\n",
    "    ...     attrs=dict(description=\"Weather related data.\"),\n",
    "    ... )\n",
    "    >>> ds\n",
    "    <xarray.Dataset>\n",
    "    Dimensions:         (x: 2, y: 2, time: 3)\n",
    "    Coordinates:\n",
    "        lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n",
    "        lat             (x, y) float64 42.25 42.21 42.63 42.59\n",
    "      * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n",
    "        reference_time  datetime64[ns] 2014-09-05\n",
    "    Dimensions without coordinates: x, y\n",
    "    Data variables:\n",
    "        temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n",
    "        precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n",
    "    Attributes:\n",
    "        description:  Weather related data.\n",
    "\n",
    "    Find out where the coldest temperature was and what values the\n",
    "    other variables had:\n",
    "\n",
    "    >>> ds.isel(ds.temperature.argmin(...))\n",
    "    <xarray.Dataset>\n",
    "    Dimensions:         ()\n",
    "    Coordinates:\n",
    "        lon             float64 -99.32\n",
    "        lat             float64 42.21\n",
    "        time            datetime64[ns] 2014-09-08\n",
    "        reference_time  datetime64[ns] 2014-09-05\n",
    "    Data variables:\n",
    "        temperature     float64 7.182\n",
    "        precipitation   float64 8.326\n",
    "    Attributes:\n",
    "        description:  Weather related data.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ds = xr.open_dataset(data_url, chunks=\"auto\")\n",
    "    VX = ds[\"vx\"]\n",
    "    VY = ds[\"vy\"]\n",
    "    VX_e = ds[\"vx_err\"]\n",
    "    VY_e = ds[\"vy_err\"]\n",
    "    x = ds[\"x\"]\n",
    "    y = ds[\"y\"]\n",
    "\n",
    "#     VX = np.squeeze(ds[\"vx\"].to_numpy())\n",
    "#     VY = np.squeeze(ds[\"vy\"].to_numpy())\n",
    "#     VX_e = np.squeeze(ds[\"vx_err\"].to_numpy())\n",
    "#     VY_e = np.squeeze(ds[\"vy_err\"].to_numpy())\n",
    "#     x = ds[\"x\"].to_numpy()\n",
    "#     y = ds[\"y\"].to_numpy()\n",
    "    \n",
    "    Vx, Vy = get_perturbed_velocities(VX, VY, VX_e, VY_e, sample=sample, sigma=sigma)\n",
    "    ogr.UseExceptions()\n",
    "    if isinstance(ogr_url, Path):\n",
    "        ogr_url = str(ogr_url.absolute())\n",
    "    in_ds = ogr.Open(ogr_url)\n",
    "\n",
    "    layer = in_ds.GetLayer(0)\n",
    "    layer_type = ogr.GeometryTypeToName(layer.GetGeomType())\n",
    "    srs = layer.GetSpatialRef()\n",
    "    srs_geo = osr.SpatialReference()\n",
    "    srs_geo.ImportFromEPSG(3413)\n",
    "\n",
    "    all_glaciers = []\n",
    "    progress = tqdm(enumerate(layer), total=len(layer), leave=False)\n",
    "    for ft, feature in progress:\n",
    "        geometry = feature.GetGeometryRef()\n",
    "        geometry.TransformTo(srs_geo)\n",
    "        points = geometry.GetPoints()\n",
    "        points = [Point(p) for p in points]\n",
    "        attrs = feature.items()\n",
    "        attrs[\"perturbation\"] = perturbation\n",
    "        glacier_name = attrs[\"name\"]\n",
    "        progress.set_description(f\"\"\"Processing {glacier_name}\"\"\")\n",
    "        trajs = []\n",
    "        for p in points:\n",
    "            traj, _ = compute_trajectory(\n",
    "                p, Vx, Vy, x, y, total_time=total_time, dt=dt, reverse=reverse\n",
    "            )\n",
    "            trajs.append(traj)\n",
    "        df = trajectories_to_geopandas(trajs, Vx, Vy, x, y, attrs=attrs)\n",
    "        all_glaciers.append(df)\n",
    "    return pd.concat(all_glaciers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f037bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perturbed_velocities(VX, VY, VX_e, VY_e, sample, sigma: float = 1.0):\n",
    "\n",
    "    VX_min, VX_max = VX - sigma * VX_e, VX + sigma * VX_e\n",
    "    VY_min, VY_max = VY - sigma * VY_e, VY + sigma * VY_e\n",
    "    \n",
    "    Vx = VX_min + sample[0] * (VX_max - VX_min)\n",
    "    Vy = VY_min + sample[1] * (VY_max - VY_min)\n",
    "    \n",
    "    return Vx, Vy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b71f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pismragis.trajectories import trajectories_to_geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3bc37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time all_perturb_glaciers = Parallel(n_jobs=10)(delayed(compute_perturbation)(data_url, ogr_url, perturbation=perturb, sample=unif_sample[perturb, :], total_time=1_000, dt=1, reverse=True) for perturb in range(n_perturbations))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b14ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time all_perturb_glaciers = Parallel(n_jobs=10)(delayed(compute_perturbation)(data_url, ogr_url, perturbation=perturb, sample=unif_sample[perturb, :], total_time=1_000, dt=1, reverse=True) for perturb in range(n_perturbations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04f8c016",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5290c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6f79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to load the extension first\n",
    "%load_ext viztracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03408a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time all_perturb_glaciers = Parallel(n_jobs=10)(delayed(compute_perturbation)(data_url, ogr_url, perturbation=perturb, sample=unif_sample[perturb, :], total_time=1_000, dt=1, reverse=True) for perturb in range(n_draw_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a85eb006",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/base/pism-ragis/notebooks/test.py:206\u001b[0m\n\u001b[1;32m    203\u001b[0m n_perturbations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    204\u001b[0m unif_sample \u001b[38;5;241m=\u001b[39m lhs(\u001b[38;5;241m2\u001b[39m, n_perturbations)\n\u001b[0;32m--> 206\u001b[0m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompute_perturbation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mogr_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mperturbation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mperturb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munif_sample\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperturb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mperturb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_perturbations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6d8f6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import xarray as xr\n",
    "from pismragis.trajectories import compute_trajectory, compute_perturbation\n",
    "from joblib import Parallel, delayed\n",
    "import geopandas as gp\n",
    "from pathlib import Path\n",
    "from pismragis.processing import tqdm_joblib\n",
    "import pandas as pd\n",
    "from pyDOE import lhs\n",
    "from scipy.stats.distributions import uniform\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Union\n",
    "\n",
    "import geopandas as gp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopandas import GeoDataFrame\n",
    "from numpy import ndarray\n",
    "from osgeo import ogr, osr\n",
    "from shapely import Point\n",
    "from tqdm.auto import tqdm\n",
    "from xarray import DataArray\n",
    "from pismragis.trajectories import trajectories_to_geopandas\n",
    "\n",
    "\n",
    "def compute_perturbation(\n",
    "    data_url: Union[str, Path],\n",
    "    ogr_url: Union[str, Path],\n",
    "    perturbation: int = 0,\n",
    "    sample: Union[list, ndarray] = [0.5, 0.5],\n",
    "    sigma: float = 1,\n",
    "    total_time: float = 10_000,\n",
    "    dt: float = 1,\n",
    "    reverse: bool = False,\n",
    ") -> GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Compute a perturbed trajectory.\n",
    "\n",
    "    It appears OGR objects cannot be pickled by joblib hence we load it here.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : string or pathlib.Path\n",
    "        Path to an ogr data set\n",
    "    VX_min : numpy.ndarray or xarray.DataArray\n",
    "        Minimum\n",
    "    VX_min : dict-like, optional\n",
    "        Another mapping in similar form as the `data_vars` argument,\n",
    "        except the each item is saved on the dataset as a \"coordinate\".\n",
    "        These variables have an associated meaning: they describe\n",
    "        constant/fixed/independent quantities, unlike the\n",
    "        varying/measured/dependent quantities that belong in\n",
    "        `variables`. Coordinates values may be given by 1-dimensional\n",
    "        arrays or scalars, in which case `dims` do not need to be\n",
    "        supplied: 1D arrays will be assumed to give index values along\n",
    "        the dimension with the same name.\n",
    "\n",
    "        The following notations are accepted:\n",
    "\n",
    "        - mapping {coord name: DataArray}\n",
    "        - mapping {coord name: Variable}\n",
    "        - mapping {coord name: (dimension name, array-like)}\n",
    "        - mapping {coord name: (tuple of dimension names, array-like)}\n",
    "        - mapping {dimension name: array-like}\n",
    "          (the dimension name is implicitly set to be the same as the\n",
    "          coord name)\n",
    "\n",
    "        The last notation implies that the coord name is the same as\n",
    "        the dimension name.\n",
    "\n",
    "    attrs : dict-like, optional\n",
    "        Global attributes to save on this dataset.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Create data:\n",
    "\n",
    "    >>> np.random.seed(0)\n",
    "    >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)\n",
    "    >>> precipitation = 10 * np.random.rand(2, 2, 3)\n",
    "    >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]\n",
    "    >>> lat = [[42.25, 42.21], [42.63, 42.59]]\n",
    "    >>> time = pd.date_range(\"2014-09-06\", periods=3)\n",
    "    >>> reference_time = pd.Timestamp(\"2014-09-05\")\n",
    "\n",
    "    Initialize a dataset with multiple dimensions:\n",
    "\n",
    "    >>> ds = xr.Dataset(\n",
    "    ...     data_vars=dict(\n",
    "    ...         temperature=([\"x\", \"y\", \"time\"], temperature),\n",
    "    ...         precipitation=([\"x\", \"y\", \"time\"], precipitation),\n",
    "    ...     ),\n",
    "    ...     coords=dict(\n",
    "    ...         lon=([\"x\", \"y\"], lon),\n",
    "    ...         lat=([\"x\", \"y\"], lat),\n",
    "    ...         time=time,\n",
    "    ...         reference_time=reference_time,\n",
    "    ...     ),\n",
    "    ...     attrs=dict(description=\"Weather related data.\"),\n",
    "    ... )\n",
    "    >>> ds\n",
    "    <xarray.Dataset>\n",
    "    Dimensions:         (x: 2, y: 2, time: 3)\n",
    "    Coordinates:\n",
    "        lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23\n",
    "        lat             (x, y) float64 42.25 42.21 42.63 42.59\n",
    "      * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08\n",
    "        reference_time  datetime64[ns] 2014-09-05\n",
    "    Dimensions without coordinates: x, y\n",
    "    Data variables:\n",
    "        temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63\n",
    "        precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805\n",
    "    Attributes:\n",
    "        description:  Weather related data.\n",
    "\n",
    "    Find out where the coldest temperature was and what values the\n",
    "    other variables had:\n",
    "\n",
    "    >>> ds.isel(ds.temperature.argmin(...))\n",
    "    <xarray.Dataset>\n",
    "    Dimensions:         ()\n",
    "    Coordinates:\n",
    "        lon             float64 -99.32\n",
    "        lat             float64 42.21\n",
    "        time            datetime64[ns] 2014-09-08\n",
    "        reference_time  datetime64[ns] 2014-09-05\n",
    "    Data variables:\n",
    "        temperature     float64 7.182\n",
    "        precipitation   float64 8.326\n",
    "    Attributes:\n",
    "        description:  Weather related data.\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ds = xr.open_dataset(data_url)\n",
    "\n",
    "#     VX = ds[\"vx\"]\n",
    "#     VY = ds[\"vy\"]\n",
    "#     VX_e = ds[\"vx_err\"]\n",
    "#     VY_e = ds[\"vy_err\"]\n",
    "#     x = ds[\"x\"]\n",
    "#     y = ds[\"y\"]\n",
    "\n",
    "    VX = np.squeeze(ds[\"vx\"].to_numpy())\n",
    "    VY = np.squeeze(ds[\"vy\"].to_numpy())\n",
    "    VX_e = np.squeeze(ds[\"vx_err\"].to_numpy())\n",
    "    VY_e = np.squeeze(ds[\"vy_err\"].to_numpy())\n",
    "    x = ds[\"x\"].to_numpy()\n",
    "    y = ds[\"y\"].to_numpy()\n",
    "\n",
    "    Vx, Vy = get_perturbed_velocities(VX, VY, VX_e, VY_e, sample=sample, sigma=sigma)\n",
    "    ogr.UseExceptions()\n",
    "    if isinstance(ogr_url, Path):\n",
    "        ogr_url = str(ogr_url.absolute())\n",
    "    in_ds = ogr.Open(ogr_url)\n",
    "\n",
    "    layer = in_ds.GetLayer(0)\n",
    "    layer_type = ogr.GeometryTypeToName(layer.GetGeomType())\n",
    "    srs = layer.GetSpatialRef()\n",
    "    srs_geo = osr.SpatialReference()\n",
    "    srs_geo.ImportFromEPSG(3413)\n",
    "\n",
    "    all_glaciers = []\n",
    "    progress = tqdm(enumerate(layer), total=len(layer), leave=False)\n",
    "    for ft, feature in progress:\n",
    "        geometry = feature.GetGeometryRef()\n",
    "        geometry.TransformTo(srs_geo)\n",
    "        points = geometry.GetPoints()\n",
    "        points = [Point(p) for p in points]\n",
    "        attrs = feature.items()\n",
    "        attrs[\"perturbation\"] = perturbation\n",
    "        glacier_name = attrs[\"name\"]\n",
    "        progress.set_description(f\"\"\"Processing {glacier_name}\"\"\")\n",
    "        trajs = []\n",
    "        for p in points:\n",
    "            traj, _ = compute_trajectory(\n",
    "                p, Vx, Vy, x, y, total_time=total_time, dt=dt, reverse=reverse\n",
    "            )\n",
    "            trajs.append(traj)\n",
    "        df = trajectories_to_geopandas(trajs, Vx, Vy, x, y, attrs=attrs)\n",
    "        all_glaciers.append(df)\n",
    "    return pd.concat(all_glaciers)\n",
    "\n",
    "\n",
    "def get_perturbed_velocities(VX, VY, VX_e, VY_e, sample, sigma: float = 1.0):\n",
    "    VX_min, VX_max = VX - sigma * VX_e, VX + sigma * VX_e\n",
    "    VY_min, VY_max = VY - sigma * VY_e, VY + sigma * VY_e\n",
    "\n",
    "    Vx = VX_min + sample[0] * (VX_max - VX_min)\n",
    "    Vy = VY_min + sample[1] * (VY_max - VY_min)\n",
    "\n",
    "    return Vx, Vy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "703b8695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel():\n",
    "    data_url = Path(\"~/Google Drive/My Drive/data/ITS_LIVE/GRE_G0240_0000.nc\")\n",
    "    ogr_url = Path(\n",
    "        \"/Users/andy/Google Drive/My Drive/data/GreenlandFluxGatesAschwanden/greenland-flux-gates-jibneighbors.shp\"\n",
    "    )\n",
    "\n",
    "    n_perturbations = 10\n",
    "    unif_sample = lhs(2, n_perturbations)\n",
    "\n",
    "    Parallel(n_jobs=10)(\n",
    "        delayed(compute_perturbation)(\n",
    "            data_url,\n",
    "            ogr_url,\n",
    "            perturbation=perturb,\n",
    "            sample=unif_sample[perturb, :],\n",
    "            total_time=1_000,\n",
    "            dt=1,\n",
    "            reverse=True,\n",
    "        )\n",
    "        for perturb in range(n_perturbations)\n",
    "    )\n",
    "\n",
    "\n",
    "def run_serial():\n",
    "    data_url = Path(\"~/Google Drive/My Drive/data/ITS_LIVE/GRE_G0240_0000.nc\")\n",
    "    ogr_url = Path(\n",
    "        \"/Users/andy/Google Drive/My Drive/data/GreenlandFluxGatesAschwanden/greenland-flux-gates-jibneighbors.shp\"\n",
    "    )\n",
    "\n",
    "    n_perturbations = 10\n",
    "    unif_sample = lhs(2, n_perturbations)\n",
    "\n",
    "    perturb = 0\n",
    "    compute_perturbation(\n",
    "        data_url,\n",
    "        ogr_url,\n",
    "        perturbation=perturb,\n",
    "        sample=unif_sample[perturb, :],\n",
    "        total_time=1_000,\n",
    "        dt=1,\n",
    "        reverse=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "682bab3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.9 s ± 240 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_serial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fafc518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.1 s ± 220 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_serial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a96e1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
