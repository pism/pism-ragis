{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43204f3d",
   "metadata": {},
   "source": [
    "# Analyze RAGIS simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de938c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from pismragis.observations import load_imbie, load_imbie_csv\n",
    "from pismragis.analysis import resample_ensemble_by_data, sensitivity_analysis\n",
    "from pismragis.processing import convert_netcdf_to_dataframe, check_file, copy_file\n",
    "from pismragis.stats import run_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d3493a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = 8\n",
    "norm_year = 1992.0\n",
    "\n",
    "plt.rc('font', size=6)\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "\n",
    "mass_varname = \"SLE (cm)\"\n",
    "mass_uncertainty_varname = \"SLE uncertainty (cm)\"\n",
    "discharge_varname = \"D (Gt/yr)\"\n",
    "discharge_uncertainty_varname = \"D uncertainty (Gt/yr)\"\n",
    "smb_varname = \"SMB (Gt/yr)\"\n",
    "smb_uncertainty_varname = \"SMB uncertainty (Gt/yr)\"\n",
    "\n",
    "\n",
    "bg_color = \"#216779\"\n",
    "bg_color = \"w\"\n",
    "sim_colors = plt.rcParams['axes.prop_cycle'].by_key()['color'][1::]\n",
    "imbie_color = plt.rcParams['axes.prop_cycle'].by_key()['color'][0]\n",
    "    \n",
    "kg2cmsle = 1 / 1e12 * 1.0 / 362.5 / 10.0\n",
    "gt2cmsle = 1 / 362.5 / 10.0\n",
    "sigma = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67ae0ff",
   "metadata": {},
   "source": [
    "## Load IMBIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1408e106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imbie = load_imbie(url=\"../imbie_dataset_greenland_dynamics-2020_02_28.xlsx\")\n",
    "imbie = load_imbie()\n",
    "# Glacier and Ice cap bias 30 Gt/yr, add it back\n",
    "imbie[mass_varname] -= 30 * gt2cmsle\n",
    "imbie[smb_varname] += 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c689537",
   "metadata": {},
   "source": [
    "glob PISM scalar time series files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5777a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../hindcasts\"\n",
    "\n",
    "exps = []\n",
    "\n",
    "exps.append({\n",
    "    \"Experiment\": \"frontal melt\",\n",
    "    \"proj_dir\": \"2023_04_ocean_calving_50\",\n",
    "    \"scalar_dir\": \"scalar\",\n",
    "    \"processed_dir\": \"processed\",\n",
    "    \"lhs\": \"gris_ragis_ocean_calving_lhs_50_w_posterior\",\n",
    "}\n",
    ")\n",
    "\n",
    "exps.append({\n",
    "    \"Experiment\": \"thickness calving\",\n",
    "    \"proj_dir\": \"2023_04_thickness_calving_50\",\n",
    "    \"scalar_dir\": \"scalar\",\n",
    "    \"processed_dir\": \"processed\",\n",
    "    \"lhs\": \"gris_ragis_thickness_calving_lhs_50_w_posterior\",\n",
    "}\n",
    ")\n",
    "\n",
    "exps.append({\n",
    "    \"Experiment\": \"no frontal melt\",\n",
    "    \"proj_dir\": \"2023_04_ocean_simple_50\",\n",
    "    \"scalar_dir\": \"scalar\",\n",
    "    \"processed_dir\": \"processed\",\n",
    "    \"lhs\": \"gris_ragis_ocean_simple_lhs_50_w_posterior\",\n",
    "}\n",
    ")\n",
    "\n",
    "experiments = []\n",
    "\n",
    "for exp in exps:\n",
    "\n",
    "    ens_file = os.path.join(data_dir, exp[\"proj_dir\"], \"uq\", f\"\"\"{exp[\"lhs\"]}.csv\"\"\")\n",
    "    s_dir = os.path.join(data_dir, exp[\"proj_dir\"], exp[\"scalar_dir\"])\n",
    "    scalar_files = sorted(glob(os.path.join(s_dir, \"ts_gris_g*00m_v20*RAGIS_id_*_1980-1-1_2020-1-1.nc\")))\n",
    "    processed_path = os.path.join(data_dir, exp[\"proj_dir\"], exp[\"processed_dir\"])\n",
    "\n",
    "    for scalar_file in scalar_files:\n",
    "        if check_file(scalar_file):\n",
    "            copy_file(scalar_file, processed_path)\n",
    "        \n",
    "    processed_files = sorted(glob(os.path.join(processed_path, \"ts_gris_g*00m_v20*RAGIS_id_*_1980-1-1_2020-1-1.nc\")))\n",
    "\n",
    "    exp_df = convert_netcdf_to_dataframe(processed_files, resample=\"yearly\", verbose=False, n_jobs=n_jobs)\n",
    "    exp_df[\"Experiment\"] = exp[\"Experiment\"]\n",
    "    exp[\"data_df\"] = exp_df\n",
    "    exp[\"run_stats\"] = run_stats(processed_files, experiment=exp[\"Experiment\"])\n",
    "    sa_df = sensitivity_analysis(exp_df.dropna(), ensemble_file=ens_file, n_jobs=n_jobs)\n",
    "    exp[\"sensitivity_df\"] = sa_df\n",
    "    experiments.append(exp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=6)\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "\n",
    "e = experiments[0][\"run_stats\"]\n",
    "m_vars = [\"processor_hours\", \"wall_clock_hours\", \"model_years_per_processor_hour\"]\n",
    "for m in range(len(experiments)):\n",
    "    e_df = experiments[m][\"run_stats\"]\n",
    "    print(f\"\"\"Peformace Summary for Experiment {experiments[m][\"Experiment\"]} ({len(e_df)} members)\"\"\")\n",
    "    print(\"---------------------------------------------------\\n\")\n",
    "    print(e_df.agg({'processor_hours': 'sum', 'wall_clock_hours': \"sum\", \"model_years_per_processor_hour\": \"mean\"}))\n",
    "    print(\"---------------------------------------------------\\n\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=len(m_vars), sharey=\"row\", figsize=[6.2, 2.0])\n",
    "fig.subplots_adjust(wspace=0)\n",
    "\n",
    "\n",
    "all_run_stats = pd.concat([experiments[k][\"run_stats\"] for k in range(len(experiments))]).reset_index(drop=True)\n",
    "[sns.histplot(data=all_run_stats, x=m_var, bins=11, kde=True, hue=\"Experiment\", ax=axs[k]) for k, m_var in enumerate(m_vars)]\n",
    "axs[0].legend([], [], frameon=False)\n",
    "axs[1].legend([], [], frameon=False)\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"run_stats.pdf\")\n",
    "!open run_stats.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e09670",
   "metadata": {},
   "source": [
    "## Plot\n",
    "\n",
    "Plot time series of cumulative mass change (cm SLE) and discharge at grounding line (Gt/yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013891f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=6)\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "\n",
    "fig, axs = plt.subplots(nrows=3, ncols=1, sharex=\"col\", figsize=(6.2, 4.2))\n",
    "fig.subplots_adjust(wspace=-0.5, hspace=-0.5)\n",
    "\n",
    "obs_ci = axs[0].fill_between(imbie[\"Date\"], \n",
    "                    (imbie[mass_varname] + sigma * imbie[mass_uncertainty_varname]), \n",
    "                    (imbie[mass_varname] - sigma * imbie[mass_uncertainty_varname]), \n",
    "                    ls=\"solid\", color=imbie_color, lw=0, alpha=0.5, label=\"observed\")\n",
    "axs[1].fill_between(imbie[\"Date\"], \n",
    "                    (imbie[discharge_varname] + sigma * imbie[discharge_uncertainty_varname]), \n",
    "                    (imbie[discharge_varname] - sigma * imbie[discharge_uncertainty_varname]), \n",
    "                    ls=\"solid\", color=imbie_color, lw=0, alpha=0.5)\n",
    "axs[2].fill_between(imbie[\"Date\"], \n",
    "                    (imbie[smb_varname] + sigma * imbie[smb_uncertainty_varname]), \n",
    "                    (imbie[smb_varname] - sigma * imbie[smb_uncertainty_varname]), \n",
    "                    ls=\"solid\", color=imbie_color, lw=0, alpha=0.5)\n",
    "\n",
    "for k, exp in enumerate(experiments):\n",
    "    df = exp[\"data_df\"]\n",
    "    q_05 = df.groupby(by=\"time\").quantile(0.05, numeric_only=True)\n",
    "    q_50 = df.groupby(by=\"time\").quantile(0.50, numeric_only=True)\n",
    "    q_95 = df.groupby(by=\"time\").quantile(0.95, numeric_only=True)\n",
    "    axs[0].plot(q_50.index, q_50[mass_varname], color=sim_colors[k], alpha=1.0, lw=1)\n",
    "    axs[1].plot(q_50.index, q_50[discharge_varname], color=sim_colors[k], alpha=1.0, lw=1)\n",
    "    axs[2].plot(q_50.index, q_50[smb_varname], color=sim_colors[k], alpha=1.0, lw=1)\n",
    "\n",
    "    axs[0].fill_between(q_05.index, q_05[mass_varname], \n",
    "                        q_95[mass_varname], color=sim_colors[k], alpha=0.4, lw=1, label=exp[\"Experiment\"])\n",
    "    axs[1].fill_between(q_05.index, q_05[discharge_varname], \n",
    "                        q_95[discharge_varname], color=sim_colors[k], alpha=0.4, lw=1)\n",
    "    axs[2].fill_between(q_05.index, q_05[smb_varname], \n",
    "                        q_95[smb_varname], color=sim_colors[k], alpha=0.4, lw=1)\n",
    "\n",
    "axs[0].plot(imbie[\"Date\"], imbie[mass_varname], color=imbie_color, lw=1)\n",
    "axs[1].plot(imbie[\"Date\"], imbie[discharge_varname], color=imbie_color, lw=1)\n",
    "axs[2].plot(imbie[\"Date\"], imbie[smb_varname], color=imbie_color, lw=1)\n",
    "\n",
    "\n",
    "\n",
    "fig.set_facecolor(bg_color)\n",
    "for ax in axs:\n",
    "    ax.set_facecolor(bg_color)\n",
    "fig.set_facecolor(bg_color)\n",
    "\n",
    "axs[0].axhline(0, color=\"k\", ls=\"dotted\", lw=1.0)\n",
    "axs[0].set_xlabel(\"\")\n",
    "axs[0].set_ylabel(\"Contribution to sea-level\\nsince 1992 (cm SLE)\")\n",
    "axs[-1].set_xlabel(\"Year\")\n",
    "axs[1].set_ylabel(\"Solid Discharge (Gt/yr)\")\n",
    "axs[2].set_ylabel(\"SMB (Gt/yr)\")\n",
    "axs[0].set_xlim(pd.to_datetime(\"1980-1-1\"), pd.to_datetime(\"2020-1-1\"))\n",
    "axs[0].set_ylim(-2.0, 2.0)\n",
    "axs[1].set_ylim(-1200, 0)\n",
    "#axs[1].set_ylim(-1000, 100)\n",
    "\n",
    "legend = axs[0].legend(loc=\"lower left\")\n",
    "legend.get_frame().set_linewidth(0.0)\n",
    "legend.get_frame().set_alpha(0.0)\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"ragis-comp_scalar.pdf\")\n",
    "!open ragis-comp_scalar.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66402b7f",
   "metadata": {},
   "source": [
    "## Plot sensitivity index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5ac98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', size=6)\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "\n",
    "\n",
    "for e in experiments:\n",
    "    sa_df = e[\"sensitivity_df\"]\n",
    "    si = \"S1\"\n",
    "    outfile = f\"\"\"{e[\"lhs\"]}.pdf\"\"\"\n",
    "    fig, axs = plt.subplots(\n",
    "        2,\n",
    "        1,\n",
    "        sharex=\"col\",\n",
    "        figsize=[6.2, 3.8],\n",
    "    )\n",
    "    fig.subplots_adjust(bottom=0.0)\n",
    "    for k, m_var in enumerate([\"limnsw (kg)\", \"grounding_line_flux (Gt year-1)\"]):\n",
    "        m_df = sa_df[sa_df[\"Variable\"] == m_var]\n",
    "        ax = axs.ravel()[k]\n",
    "        p_df = m_df[m_df[\"Si\"] == si].drop(columns=[\"Si\", \"Variable\"]).set_index(\"Date\")\n",
    "        p_conf_df = m_df[m_df[\"Si\"] == si + \"_conf\"].drop(columns=[\"Si\"])\n",
    "\n",
    "        [\n",
    "            ax.plot(p_df.index, p_df[v], lw=1, label=v)\n",
    "            for v in sa_df.drop(columns=[\"Si\", \"Variable\", \"Date\"]).keys()\n",
    "        ]\n",
    "\n",
    "        [\n",
    "            ax.fill_between(\n",
    "                p_df.index,\n",
    "                p_df[v].values - p_conf_df[v].values,\n",
    "                p_df[v].values + p_conf_df[v].values,\n",
    "                alpha=0.2,\n",
    "                lw=0,\n",
    "            )\n",
    "            for v in sa_df.drop(columns=[\"Si\", \"Variable\", \"Date\"]).keys()\n",
    "        ]\n",
    "        ax.set_xlim(datetime(1980, 1, 1), datetime(2020, 1, 1))\n",
    "        lgd = ax.set_title(f\"{si} indices for '{m_var}'\")\n",
    "    legend = axs[-1].legend(loc=\"lower left\", ncols=3, bbox_to_anchor=(0, -0.75))\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"{si}_{outfile}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef451a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_e = experiments[2][\"data_df\"]\n",
    "ragis_calib, weights = resample_ensemble_by_data(imbie, m_e, \n",
    "                                        fudge_factor=3, verbose=True)\n",
    "calib_exps = ragis_calib[\"id\"].unique()\n",
    "print(calib_exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d852e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=2, ncols=1, sharex=\"col\", figsize=(12, 8))\n",
    "fig.subplots_adjust(wspace=0.0, hspace=0.0)\n",
    "\n",
    "q_5 = ragis_calib.groupby(by=\"Year\").quantile(0.05, numeric_only=True)\n",
    "q_16 = ragis_calib.groupby(by=\"Year\").quantile(0.16, numeric_only=True)\n",
    "q_50 = ragis_calib.groupby(by=\"Year\").quantile(0.50, numeric_only=True)\n",
    "q_84 = ragis_calib.groupby(by=\"Year\").quantile(0.84, numeric_only=True)\n",
    "q_95 = ragis_calib.groupby(by=\"Year\").quantile(0.95, numeric_only=True)\n",
    "for exp in m_e:\n",
    "    print(exp)\n",
    "    e = m_e[m_e[\"id\"] == exp]\n",
    "    axs[0].plot(e[\"Year\"], e[mass_varname], color=\"k\", lw=1)\n",
    "    print(e)\n",
    "    axs[1].plot(e[\"Year\"], e[discharge_varname], color=\"k\", lw=1)\n",
    "\n",
    "for exp in calib_exps:\n",
    "    e = m_e[m_e[\"id\"] == exp]\n",
    "    axs[0].plot(e[\"Year\"], e[mass_varname], lw=1)\n",
    "    axs[1].plot(e[\"Year\"], e[discharge_varname], lw=1)\n",
    "\n",
    "axs[0].fill_between(q_50.index, q_16[mass_varname], q_84[mass_varname], color=\"w\")\n",
    "axs[0].fill_between(imbie[\"Year\"], \n",
    "                    (imbie[mass_varname] + sigma * imbie[mass_uncertainty_varname]), \n",
    "                    (imbie[mass_varname] - sigma * imbie[mass_uncertainty_varname]), \n",
    "                    ls=\"solid\", lw=0, alpha=0.35, label=\"2-$\\sigma$ IMBIE\")\n",
    "axs[1].fill_between(imbie[\"Year\"], \n",
    "                    (imbie[discharge_varname] + sigma * imbie[discharge_uncertainty_varname]), \n",
    "                    (imbie[discharge_varname] - sigma * imbie[discharge_uncertainty_varname]), \n",
    "                    ls=\"solid\", lw=0, alpha=0.35, color=imbie_color)\n",
    "\n",
    "\n",
    "axs[0].set_xlim(1992, 2020)\n",
    "axs[0].set_ylim(0, 2)\n",
    "fig.set_facecolor(bg_color)\n",
    "for ax in axs:\n",
    "    ax.set_facecolor(bg_color)\n",
    "fig.set_facecolor(bg_color)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34d7026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c763e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([  634.45744104  -235.12952599 -1167.45917464  -531.87414125\n",
    "  -904.57676904    91.04033758   722.82730577   114.16742036\n",
    "   423.55949261   625.02532308  -293.90580578   647.25631402\n",
    "  -586.31879794 -1381.01064481   349.68201297   716.26847793\n",
    "   363.33248697 -1011.23876497   -48.85983816   406.46475582\n",
    "   605.17371227  -949.29602232  -988.06082517  -534.95763144\n",
    "   405.57114816  -986.00865133  -649.38528098   665.06762932\n",
    "   625.46368323   385.91494928 -1067.25259603  1053.83389006\n",
    "   230.25552736  -390.24453682   176.72057353 -1066.75620664\n",
    "  -303.09711807   312.49178881   942.86270865   783.1536109\n",
    "   111.79503873  -139.82581505   570.4991376   -757.75426472\n",
    "   586.62399324    39.56232318   879.51778518   524.42354351]\n",
    "[  634.45744104  -235.12952599 -1167.45917464  -531.87414125\n",
    "  -904.57676904    91.04033758   722.82730577   114.16742036\n",
    "   423.55949261   625.02532308  -293.90580578   647.25631402\n",
    "  -586.31879794 -1381.01064481   349.68201297   716.26847793\n",
    "   363.33248697 -1011.23876497   -48.85983816   406.46475582\n",
    "   605.17371227  -949.29602232  -988.06082517  -534.95763144\n",
    "   405.57114816  -986.00865133  -649.38528098   665.06762932\n",
    "   625.46368323   385.91494928 -1067.25259603  1053.83389006\n",
    "   230.25552736  -390.24453682   176.72057353 -1066.75620664\n",
    "  -303.09711807   312.49178881   942.86270865   783.1536109\n",
    "   111.79503873  -139.82581505   570.4991376   -757.75426472\n",
    "   586.62399324    39.56232318   879.51778518   524.42354351])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb63ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebb3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cbd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e77bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554553ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56b898",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607ee8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments[1][\"data_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87ceb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_e = experiments[2][\"data_df\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b8caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b028bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_ensemble_by_data_o(\n",
    "    observed: pd.DataFrame,\n",
    "    simulated: pd.DataFrame,\n",
    "    id_var: str = \"id\",\n",
    "    calibration_start: float = 1992.0,\n",
    "    calibration_end: float = 2017.0,\n",
    "    fudge_factor: float = 3,\n",
    "    n_samples: int = 100,\n",
    "    verbose: bool = False,\n",
    "    m_var: str = \"Mass (Gt)\",\n",
    "    m_var_std: str = \"Mass uncertainty (Gt)\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resampling algorithm by Douglas C. Brinkerhoff\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observed : pandas.DataFrame\n",
    "        A dataframe with observations\n",
    "    simulated : pandas.DataFrame\n",
    "        A dataframe with simulations\n",
    "    calibration_start : float\n",
    "        Start year for calibration\n",
    "    calibration_end : float\n",
    "        End year for calibration\n",
    "    fudge_factor : float\n",
    "        Tolerance for simulations. Calculated as fudge_factor * standard deviation of observed\n",
    "    n_samples : int\n",
    "        Number of samples to draw.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    observed_calib_time = (observed[\"Year\"] >= calibration_start) & (\n",
    "        observed[\"Year\"] <= calibration_end\n",
    "    )\n",
    "    observed_calib_period = observed[observed_calib_time]\n",
    "    observed_interp_mean = interp1d(\n",
    "        observed_calib_period[\"Year\"], observed_calib_period[m_var]\n",
    "    )\n",
    "    observed_interp_std = interp1d(\n",
    "        observed_calib_period[\"Year\"], observed_calib_period[m_var_std]\n",
    "    )\n",
    "    simulated_calib_time = (simulated[\"Year\"] >= calibration_start) & (\n",
    "        simulated[\"Year\"] <= calibration_end\n",
    "    )\n",
    "    simulated_calib_period = simulated[simulated_calib_time]\n",
    "\n",
    "    resampled_list = []\n",
    "    log_likes = []\n",
    "    experiments = sorted(simulated_calib_period[id_var].unique())\n",
    "    evals = []\n",
    "    for i in experiments:\n",
    "        exp_ = simulated_calib_period[(simulated_calib_period[id_var] == i)]\n",
    "        exp_interp = interp1d(exp_[\"Year\"], exp_[m_var])\n",
    "        log_like = 0.0\n",
    "        for year, observed_mean, observed_std in zip(\n",
    "            observed_calib_period[\"Year\"],\n",
    "            observed_calib_period[m_var],\n",
    "            observed_calib_period[m_var_std],\n",
    "        ):\n",
    "            try:\n",
    "                observed_std *= fudge_factor\n",
    "                exp_mean = exp_interp(year)\n",
    "\n",
    "                log_like -= 0.5 * (\n",
    "                    (exp_mean - observed_mean) / observed_std\n",
    "                ) ** 2 + 0.5 * np.log(2 * np.pi * observed_std**2)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if log_like != 0:\n",
    "            evals.append(i)\n",
    "            log_likes.append(log_like)\n",
    "            if verbose:\n",
    "                print(f\"Experiment {i:.0f}: {log_like:.2f}\")\n",
    "    experiments = np.array(evals)\n",
    "    w = np.array(log_likes)\n",
    "    w -= w.mean()\n",
    "    weights = np.exp(w)\n",
    "    weights /= weights.sum()\n",
    "    resampled_experiments = np.random.choice(experiments, n_samples, p=weights)\n",
    "    new_frame = []\n",
    "    for i in resampled_experiments:\n",
    "        new_frame.append(simulated[(simulated[id_var] == i)])\n",
    "    simulated_resampled = pd.concat(new_frame)\n",
    "    resampled_list.append(simulated_resampled)\n",
    "\n",
    "    simulated_resampled = pd.concat(resampled_list)\n",
    "\n",
    "    return simulated_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bd9906",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_ensemble_by_data(\n",
    "    observed: pd.DataFrame,\n",
    "    simulated: pd.DataFrame,\n",
    "    id_var: str = \"id\",\n",
    "    calibration_start: float = 1992.0,\n",
    "    calibration_end: float = 2017.0,\n",
    "    fudge_factor: float = 3,\n",
    "    n_samples: int = 100,\n",
    "    verbose: bool = False,\n",
    "    m_var: str = \"Mass (Gt)\",\n",
    "    m_var_std: str = \"Mass uncertainty (Gt)\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Resampling algorithm by Douglas C. Brinkerhoff\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observed : pandas.DataFrame\n",
    "        A dataframe with observations\n",
    "    simulated : pandas.DataFrame\n",
    "        A dataframe with simulations\n",
    "    calibration_start : float\n",
    "        Start year for calibration\n",
    "    calibration_end : float\n",
    "        End year for calibration\n",
    "    fudge_factor : float\n",
    "        Tolerance for simulations. Calculated as fudge_factor * standard deviation of observed\n",
    "    n_samples : int\n",
    "        Number of samples to draw.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    observed_calib_time = (observed[\"Year\"] >= calibration_start) & (\n",
    "        observed[\"Year\"] <= calibration_end\n",
    "    )\n",
    "    observed_calib_period = observed[observed_calib_time]\n",
    "    observed_interp_mean = interp1d(\n",
    "        observed_calib_period[\"Year\"], observed_calib_period[m_var]\n",
    "    )\n",
    "    observed_interp_std = interp1d(\n",
    "        observed_calib_period[\"Year\"], observed_calib_period[m_var_std]\n",
    "    )\n",
    "    simulated_calib_time = (simulated[\"Year\"] >= calibration_start) & (\n",
    "        simulated[\"Year\"] <= calibration_end\n",
    "    )\n",
    "    simulated_calib_period = simulated[simulated_calib_time]\n",
    "\n",
    "    resampled_list = []\n",
    "    log_likes = []\n",
    "    experiments = sorted(simulated_calib_period[id_var].unique())\n",
    "    evals = []\n",
    "    for i in experiments:\n",
    "        exp_ = simulated_calib_period[(simulated_calib_period[id_var] == i)]\n",
    "        exp_interp = interp1d(exp_[\"Year\"], exp_[m_var])\n",
    "        log_like = 0.0\n",
    "        for year, exp_mean in zip(exp_[\"Year\"], exp_[m_var]):\n",
    "            try:\n",
    "                observed_mean = observed_interp_mean(year)\n",
    "                observed_std = observed_interp_std(year) * fudge_factor\n",
    "                log_like -= 0.5 * (\n",
    "                    (exp_mean - observed_mean) / observed_std\n",
    "                ) ** 2 + 0.5 * np.log(2 * np.pi * observed_std**2)\n",
    "                print(i, year, f\"{observed_mean:.3f}\", f\"{observed_std:.3f}\", f\"{exp_mean:.3f}\")\n",
    "\n",
    "            except ValueError:\n",
    "                pass\n",
    "        if log_like != 0:\n",
    "            evals.append(i)\n",
    "            log_likes.append(log_like)\n",
    "            if verbose:\n",
    "                print(f\"Experiment {i:.0f}: {log_like:.2f}\")\n",
    "    experiments = np.array(evals)\n",
    "    w = np.array(log_likes)\n",
    "    w -= w.mean()\n",
    "    weights = np.exp(w)\n",
    "    weights /= weights.sum()\n",
    "    resampled_experiments = np.random.choice(experiments, n_samples, p=weights)\n",
    "    new_frame = []\n",
    "    for i in resampled_experiments:\n",
    "        new_frame.append(simulated[(simulated[id_var] == i)])\n",
    "    simulated_resampled = pd.concat(new_frame)\n",
    "    resampled_list.append(simulated_resampled)\n",
    "\n",
    "    simulated_resampled = pd.concat(resampled_list)\n",
    "\n",
    "    return simulated_resampled, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660a0ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4f8905e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "found the following matches with the input file in xarray's IO backends: ['netcdf4', 'h5netcdf']. But their dependencies may not be installed, see:\nhttps://docs.xarray.dev/en/stable/user-guide/io.html \nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../../pism-emulator/pddemulator/DMI-HIRHAM5_1980_2020_MMS.nc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xarray/backends/api.py:523\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    520\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[1;32m    522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 523\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m backend \u001b[38;5;241m=\u001b[39m plugins\u001b[38;5;241m.\u001b[39mget_backend(engine)\n\u001b[1;32m    527\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[1;32m    528\u001b[0m     decode_cf,\n\u001b[1;32m    529\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[1;32m    536\u001b[0m )\n",
      "File \u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/xarray/backends/plugins.py:164\u001b[0m, in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound the following matches with the input file in xarray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms IO \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackends: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompatible_engines\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. But their dependencies may not be installed, see:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/user-guide/io.html \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m     )\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: found the following matches with the input file in xarray's IO backends: ['netcdf4', 'h5netcdf']. But their dependencies may not be installed, see:\nhttps://docs.xarray.dev/en/stable/user-guide/io.html \nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html"
     ]
    }
   ],
   "source": [
    "xr.open_dataset(\"../../pism-emulator/pddemulator/DMI-HIRHAM5_1980_2020_MMS.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae40de11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
